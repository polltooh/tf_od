{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using tf.keras in eager execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/images/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/images/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/images/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will discuss how to detect objects in images. We'll build an object detection neural network using tf.keras in eager execution mode.\n",
    "\n",
    "## Specific concepts that will be covered:\n",
    "In the process, we will build practical experience and develop intuition around the following concepts\n",
    "\n",
    "* Builds model with tf.keras pretrained model. \n",
    "* Multi-task (classification and regression) learning. \n",
    "* Gradient backpropagation in the eager mode. \n",
    "\n",
    "## Things are important for understanding the object detection pipeline. You can check the source code for more details:\n",
    "* Convert object detection dataset with json format into tfrecords and load with tf.data\n",
    "* Load tfrecords with tf.data\n",
    "* Anchor box generations and bounding box operations\n",
    "* Loss computation with hard negative mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "\n",
    "from object_detection_lib import citycam_dataset_converter\n",
    "from object_detection_lib import anchor_lib\n",
    "from object_detection_lib import dataset_lib\n",
    "from object_detection_lib import bbox_lib\n",
    "from object_detection_lib import model_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the tensorflow log.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Initialize the tensorflow eager mode\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset\n",
    "\n",
    "For this tutorial purpose, we are using public dataset [citycam](www.citycam-cmu.com). It is release from paper [Understanding Traffic Density from Large-Scale Web Camera Data](https://arxiv.org/abs/1703.05868). The dataset contains a list of images and annotation json file, which is widely used for object detection task. The library code contains the method to convert those format to tfrecord, which is used in tensorflow, and load them with tf.data. \n",
    "\n",
    "Please follow the [link](https://www.citycam-cmu.com/dataset) and click on Download Sample Data buttom. It will direct you to a google drive. Please download the tar file and put the downloaded path in the tarfile_path below.\n",
    "\n",
    "The dataset contains images, bounding box for each vehicle and mask for the specific scene. The annotation only contains the vehicle in the mask region. When training the neural network, the mask will be used to filter out the loss outside of the mask.\n",
    "\n",
    "The annotation json file contains a list of dictionary with key: **labels, mask_name, image_name, bboxes**. The mask name and image_name are the relative path to the dataset dir. \n",
    "\n",
    "Image with bounding box| Mask\n",
    "- | -\n",
    "![image](images/od_image.png) | ![mask](images/od_mask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarfile_path = \"164.tar.gz\" # Replace with your downloaded path.\n",
    "\n",
    "if not os.path.exists(tarfile_path):\n",
    "    print(\"Please make sure the tarfile you entered is current.\")\n",
    "    exit(1)\n",
    "\n",
    "# Convert the dataset.\n",
    "train_filepath, val_filepath = citycam_dataset_converter.convert(tarfile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate anchor boxes for the network.\n",
    "\n",
    "Multiple anchor boxes (filters) is introduced for object detection in the paper: [Faster R-CNN: Towards Real-Time Object\n",
    "Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf). It helps the network for detecting objects at different scales.\n",
    "\n",
    "![anchor](images/od_anchor1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_h = 15\n",
    "output_w = 22\n",
    "input_shape_h = 240\n",
    "input_shape_w = 352\n",
    "\n",
    "anchor_strides = [input_shape_h / output_h, input_shape_w / output_w]\n",
    "\n",
    "anchors = anchor_lib.anchor_gen(output_h, output_w, anchor_stride=anchor_strides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the parameters for loading the data\n",
    "\n",
    "train_ds and val_ds are initialized with tf.data. It is a dictionary:\n",
    "* **bboxes**: bounding box.\n",
    "* **labels**: labels.\n",
    "* **image**: decoded images.\n",
    "* **mask**: decoded masks.\n",
    "* **bboxes_preprocessed**: preprocessed bounding boxes. It is generated relative to the anchor boxes.\n",
    "* **labels_preprocessed**: preprocessed labels. It is generated relative to the anchor boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "pos_iou_threshold = 0.7\n",
    "neg_iou_threshold = 0.3\n",
    "neg_label_value = -1\n",
    "ignore_label_value = -2\n",
    "\n",
    "dataset_builder_fn = functools.partial(\n",
    "    dataset_lib.read_data,\n",
    "    anchors=anchors,\n",
    "    batch_size=batch_size,\n",
    "    pos_iou_threshold=pos_iou_threshold,\n",
    "    neg_iou_threshold=neg_iou_threshold,\n",
    "    neg_label_value=neg_label_value,\n",
    "    ignore_label_value=ignore_label_value)\n",
    "\n",
    "epoch = 30\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "train_ds = dataset_builder_fn(\n",
    "    train_filepath, epoch=epoch,\n",
    "    shuffle_buffer_size=shuffle_buffer_size,\n",
    "    image_arg=True)\n",
    "\n",
    "val_ds = dataset_builder_fn(val_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "Due to the small sample data size, we are using pretrained model ResNet50 for feature extraction. Classification branch and regression branch are added after the ResNet50. Classification branch predicts the likelihood of certain objects at a certain localtion, and the regression branch predicts the size of the objects. Please see region proposal network in [Faster R-CNN: Towards Real-Time Object\n",
    "Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf) and [SSD: Single Shot MultiBox Detector](https://arxiv.org/pdf/1512.02325.pdf) for more details.\n",
    "\n",
    "![](images/od_branches2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes, anchor_num_per_output):\n",
    "    base_network_model = tf.keras.applications.resnet50.ResNet50(\n",
    "        include_top=False, weights=\"imagenet\")\n",
    "\n",
    "    for layer in base_network_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    h = base_network_model.get_layer(name=\"activation_39\").output\n",
    "    drop_rate = 0.5\n",
    "    h = tf.keras.layers.Dropout(drop_rate)(h)\n",
    "\n",
    "    classification_branch = tf.keras.layers.Conv2D(\n",
    "        (num_classes + 1) * anchor_num_per_output, (1, 1))(\n",
    "            h)\n",
    "    regression_branch = tf.keras.layers.Conv2D(4 * anchor_num_per_output, (1, 1))(\n",
    "        h)\n",
    "    model_outputs = [classification_branch, regression_branch]\n",
    "    return tf.keras.models.Model(base_network_model.input, model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Specify training configuration and parameters\n",
    "\n",
    "The model will be saved in \"model\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "# Number of class in the dataset.\n",
    "num_classes = 10\n",
    "\n",
    "# length of anchor scales (3) * length of aspect ratio (3)\n",
    "anchor_num_per_output = 9\n",
    "\n",
    "# Initialize the model\n",
    "od_model = build_model(num_classes, anchor_num_per_output)\n",
    "\n",
    "# Initialize for the learning.\n",
    "learning_rate = 0.001\n",
    "decay_step = 1000\n",
    "decay_alpha = 0.000001\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "decayed_lr = tf.train.cosine_decay(\n",
    "    learning_rate=learning_rate,\n",
    "    global_step=global_step,\n",
    "    decay_steps=decay_step,\n",
    "    alpha=decay_alpha)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(decayed_lr)\n",
    "\n",
    "# Initialize for the loss.\n",
    "classificaiton_loss_weight = 1\n",
    "regression_loss_weight = 10\n",
    "negative_ratio = 3\n",
    "\n",
    "compute_loss_fn = functools.partial(\n",
    "    model_lib.compute_loss,\n",
    "    num_classes=num_classes,\n",
    "    c_weight=classificaiton_loss_weight,\n",
    "    r_weight=regression_loss_weight,\n",
    "    neg_label_value=neg_label_value,\n",
    "    ignore_label_value=ignore_label_value,\n",
    "    negative_ratio=negative_ratio)\n",
    "\n",
    "# Initialize parameters for training loop.\n",
    "val_iter = 100\n",
    "val_batch = 5\n",
    "test_iter = 500\n",
    "test_batch = 10\n",
    "score_threshold = 0.5\n",
    "max_prediction = 100\n",
    "\n",
    "train_loss_sum = 0\n",
    "\n",
    "model_dir = \"models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, train_item in enumerate(train_ds):\n",
    "    with tf.GradientTape() as tape:\n",
    "        print(train_index)\n",
    "        train_network_output = od_model(train_item[\"image\"], training=True)\n",
    "        train_loss = compute_loss_fn(train_network_output,\n",
    "                                           train_item[\"bboxes_preprocessed\"],\n",
    "                                           train_item[\"labels_preprocessed\"])\n",
    "        train_loss_sum += train_loss\n",
    "\n",
    "        grads = tape.gradient(train_loss, od_model.variables)\n",
    "        optimizer.apply_gradients(\n",
    "            zip(grads, od_model.variables),\n",
    "            global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    if train_index != 0 and train_index % val_iter == 0:\n",
    "        val_loss_sum = 0\n",
    "        for val_index, val_item in enumerate(val_ds):\n",
    "            if val_index != 0 and val_index % val_batch == 0:\n",
    "                break\n",
    "            val_network_output = od_model(val_item[\"image\"], training=False)\n",
    "            val_loss = compute_loss_fn(val_network_output,\n",
    "                                             val_item[\"bboxes_preprocessed\"],\n",
    "                                             val_item[\"labels_preprocessed\"])\n",
    "            val_loss_sum += val_loss\n",
    "\n",
    "        train_loss = train_loss_sum / val_iter\n",
    "        val_loss = val_loss_sum / val_batch\n",
    "\n",
    "        print(\"Loss at step {:04d}: train loss: {:.3f}, val loss: {:3f}\".format(\n",
    "            train_index, train_loss, val_loss))\n",
    "\n",
    "        train_loss_sum = 0\n",
    "\n",
    "od_model.save_weights(os.path.join(model_dir, \"od_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Load the model saved from previous step. The results will be saved in the \"results\" directory. By default it will save 20 results. You can adjust the save_image_number to change the number of images to save. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_model.load_weights(os.path.join(model_dir, \"od_model\"))\n",
    "test_ds = dataset_builder_fn(val_filepath)\n",
    "\n",
    "# Number of images saved in the testing process.\n",
    "save_image_number = 20\n",
    "\n",
    "save_image_count = 0\n",
    "save_image_dir = \"results\"\n",
    "if not os.path.exists(save_image_dir):\n",
    "    os.makedirs(save_image_dir)\n",
    "\n",
    "for test_index, test_item in enumerate(test_ds):\n",
    "    if save_image_count == save_image_number:\n",
    "      break\n",
    "\n",
    "    test_network_output = od_model(test_item[\"image\"], training=False)\n",
    "    bbox_list, label_list = model_lib.predict(\n",
    "        test_network_output,\n",
    "        mask=test_item[\"mask\"],\n",
    "        score_threshold=score_threshold,\n",
    "        neg_label_value=neg_label_value,\n",
    "        anchors=anchors,\n",
    "        max_prediction=max_prediction,\n",
    "        num_classes=num_classes)\n",
    "\n",
    "    for image, bbox, label in zip(test_item[\"image\"], bbox_list, label_list):\n",
    "        # label is converted to [0, 9] for training. +1 to match the original label map [1, 10].\n",
    "        label_list = [label + 1 for label in label_list]\n",
    "        # Image is whitened in the preprocess.\n",
    "        image += 0.5\n",
    "        normalized_bboxes = bbox_lib.normalizing_bbox(\n",
    "            bbox, input_shape_h, input_shape_w)\n",
    "        image_with_bboxes = tf.image.draw_bounding_boxes(\n",
    "            image[tf.newaxis, ...], normalized_bboxes[tf.newaxis, ...])\n",
    "        image_with_bboxes = tf.image.encode_png(tf.cast(image_with_bboxes[0] * 255, tf.uint8))\n",
    "        filepath = tf.constant(os.path.join(save_image_dir, '{}.png'.format(save_image_count)))\n",
    "        tf.write_file(filepath, image_with_bboxes)\n",
    "        save_image_count += 1\n",
    "        if save_image_count == save_image_number:\n",
    "          break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "|result 0 |result 1 | result 2 \n",
    "|:- | :- | :-\n",
    "|![alt](images/od_result_0.png) | ![alt](images/od_result_1.png) | ![alt](images/od_result_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
